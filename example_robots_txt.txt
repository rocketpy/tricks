Составляем файл robots.txt

robots.txt — это текстовый файл, который содержит параметры индексирования сайта для поисковых роботов.
С помощью этого файла и специальных инструкций в нём (директив) можно управлять индексацией сайта. 
Грубо говоря, с robots.txt мы можем сообщать роботу поисковой системы, какие страницы можно индексировать, а какие нельзя.

Две директивы для данного файла обязательны: это User-agent и Allow либо Disallow. Необязательной, но часто встречающейся директивой является Sitemap.

В User-agent указывается поисковой робот, к которому относятся команды из файла.
Примеры: User-agent: Yandex, User-agent: Googlebot, User-agent: * (включает все боты).
Disallow запрещает индексировать прописанные в нём отдельные страницы или целые разделы. Примеры: Disallow: /catalog/, Disallow: /catalog/page.html.
Allow, напротив, разрешает индексировать разделы и страницы сайта. Работает для всего сайта по умолчанию, если обратное не обозначено директивой Disallow. Чаще всего Allow используется в связке с Disallow, когда одну часть раздела нужно закрыть от индексации, а другую — открыть.
Sitemap содержит ссылку на карту сайта, где прописана вся его структура. Пример: Sitemap: sitemap.xml.
Пример составленного файла robots.txt для всех поисковых систем со всеми открытыми страницами и картой сайта:

User-agent: *
Allow: /
Sitemap: sitemap.xml
